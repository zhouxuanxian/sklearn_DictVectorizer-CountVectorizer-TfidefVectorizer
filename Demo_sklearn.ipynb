{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city=beijing', 'city=shanghai', 'city=shenzhen', 'temperature']\n",
      "[[  1.   0.   0. 100.]\n",
      " [  0.   1.   0.  60.]\n",
      " [  0.   0.   1.  30.]]\n",
      "[{'city=beijing': 1.0, 'temperature': 100.0}, {'city=shanghai': 1.0, 'temperature': 60.0}, {'city=shenzhen': 1.0, 'temperature': 30.0}]\n",
      "[[  1.   0.   0. 100.]\n",
      " [  0.   1.   0.  60.]\n",
      " [  0.   0.   1.  30.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "def dictvector():\n",
    "    dict = DictVectorizer(sparse=False)\n",
    "    x=[{'city':'beijing','temperature':100},{'city':'shanghai','temperature':60},{'city':'shenzhen','temperature':30}]\n",
    "    data = dict.fit_transform(x)\n",
    "    print(dict.get_feature_names())\n",
    "    print(data)\n",
    "    print(dict.inverse_transform(data))\n",
    "    print(dict.transform(x))\n",
    "    return None\n",
    "if __name__ == '__main__':\n",
    "    dictvector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dislike', 'is', 'life', 'like', 'long', 'python', 'short']\n",
      "[[0 2 1 1 0 1 1]\n",
      " [1 1 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def countvect():\n",
    "    cv = CountVectorizer()\n",
    "    X = [\"life is is short,i like python\",\"life is long,i dislike python\"]\n",
    "    data = cv.fit_transform(X)\n",
    "    print(cv.get_feature_names())\n",
    "    print(data.toarray())\n",
    "countvect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['不止', '今天', '心有', '明天', '残酷', '生活', '眼前', '舞台', '苟且', '还有', '远方']\n",
      "[[0 1 0 1 2 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 1 1 0 1 1 1]\n",
      " [0 0 1 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "def cutWord():\n",
    "    con1 = jieba.cut(\"今天很残酷，明天更残酷！\")\n",
    "    content1 =list(con1)\n",
    "    c1 = ' '.join(content1)\n",
    "    con2 =jieba.cut(\"生活不止眼前和苟且，还有诗与远方！\")\n",
    "    content2 = list(con2)\n",
    "    c2 = ' '.join(content2)\n",
    "    con3 = jieba.cut(\"心有多大，舞台就有多大！\")\n",
    "    content3 = list(con3)\n",
    "    c3 = ' '.join(content3)\n",
    "    return c1,c2,c3\n",
    "def countvect():\n",
    "    cv = CountVectorizer()\n",
    "    c1,c2,c3 = cutWord()\n",
    "    data = cv.fit_transform([c1,c2,c3])\n",
    "    print(cv.get_feature_names())\n",
    "    print(data.toarray())\n",
    "countvect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/78/sdr7ph010819mkzfc5wp37s00000gn/T/jieba.cache\n",
      "Loading model cost 1.099 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['不止', '今天', '心有', '明天', '残酷', '生活', '眼前', '舞台', '苟且', '还有', '远方']\n",
      "[[0.         0.40824829 0.         0.40824829 0.81649658 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.40824829 0.         0.         0.         0.         0.40824829\n",
      "  0.40824829 0.         0.40824829 0.40824829 0.40824829]\n",
      " [0.         0.         0.70710678 0.         0.         0.\n",
      "  0.         0.70710678 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "def cutWord():\n",
    "    con1 = jieba.cut(\"今天很残酷，明天更残酷！\")\n",
    "    content1 =list(con1)\n",
    "    c1 = ' '.join(content1)\n",
    "    con2 =jieba.cut(\"生活不止眼前和苟且，还有诗与远方！\")\n",
    "    content2 = list(con2)\n",
    "    c2 = ' '.join(content2)\n",
    "    con3 = jieba.cut(\"心有多大，舞台就有多大！\")\n",
    "    content3 = list(con3)\n",
    "    c3 = ' '.join(content3)\n",
    "    return c1,c2,c3\n",
    "def tfidfvect():\n",
    "    tf = TfidfVectorizer()\n",
    "    c1,c2,c3 = cutWord()\n",
    "    data = tf.fit_transform([c1,c2,c3])\n",
    "    print(tf.get_feature_names())\n",
    "    print(data.toarray())\n",
    "tfidfvect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
